{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d33e014-e1f0-4186-9313-de4fc61b8acb",
   "metadata": {},
   "source": [
    "# Phase 2 - Data Preprocessing and Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ca521-c74e-41fe-9dca-0110ddd34513",
   "metadata": {},
   "source": [
    " Step 1 - Data Ingestion & Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135ca6f-8322-4bf4-ba92-fd6bbef84306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from C:\\Users\\ali01\\Documents\\Ali\\Final year PJ\\Execution\\integrated_traffic_dataset (1).csv\n",
      "Original shape (rows, columns): (20085, 24)\n",
      "Removed 0 duplicate rows.\n",
      "Shape after removing duplicates: (20085, 24)\n",
      "Dropped redundant columns. Final shape: (20085, 24)\n",
      "--- Remaining Columns ---\n",
      "['record_id', 'Timestamp', 'Location', 'Latitude', 'Longitude', 'Day_of_Week', 'Hour_of_Day', 'Incident_Congestion', 'Weather_Condition', 'Road_Condition', 'Vehicle_Type', 'Vehicle_Damage', 'Accident_Severity', 'Accident_Reason', 'Driver_Violation', 'Number_of_Deaths', 'Number_of_Injuries', 'Summary_Road_Name', 'Daily_Traffic_Volume', 'Daily_Average_Speed', 'Daily_Congestion_Level', 'Daily_Incident_Reports', 'Daily_Public_Transport_Usage', 'Daily_Parking_Usage']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Loading the dataset\n",
    "# Make sure this path is correct for your local machine\n",
    "file_path = r\"Datasets\\integrated_traffic_dataset (1).csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded data from {file_path}\")\n",
    "    print(f\"Original shape (rows, columns): {df.shape}\")\n",
    "\n",
    "    # 2. Remove all exact duplicate rows\n",
    "    original_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {original_rows - len(df)} duplicate rows.\")\n",
    "    print(f\"Shape after removing duplicates: {df.shape}\")\n",
    "\n",
    "    # 3. Droping clearly redundant columns\n",
    "    redundant_cols = [\n",
    "        'accident_reference', \n",
    "        'accident_year'\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(columns=redundant_cols, errors='ignore')\n",
    "    print(f\"Dropped redundant columns. Final shape: {df.shape}\")\n",
    "    print(\"--- Remaining Columns ---\")\n",
    "    print(df.columns.to_list())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\" ERROR: File not found at {file_path}\")\n",
    "    print(\"Please double-check the file path and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c394fc-253e-48eb-9949-ebd2a52c41b7",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________________________________\n",
    "Step 2: Data Type & Error Correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a69a239-e7d2-4d01-943d-771b22b0be9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Correcting Data Types ---\n",
      "'Timestamp' column converted to datetime.\n",
      "'Latitude' column converted from float64 to numeric.\n",
      "'Longitude' column converted from float64 to numeric.\n",
      "Info: Column 'Number_of_Vehicles' not found, skipping conversion.\n",
      "Info: Column 'Number_of_Casualties' not found, skipping conversion.\n",
      "\n",
      "--- 2. Marking Invalid Placeholders as NaN ---\n",
      "ℹInfo: 'Speed_Limit' column not found, skipping.\n",
      "'Latitude': Marked -1 and 0 as NaN.\n",
      "'Longitude': Marked -1 and 0 as NaN.\n",
      "\n",
      "--- Data Types After Correction ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20085 entries, 0 to 20084\n",
      "Data columns (total 24 columns):\n",
      " #   Column                        Non-Null Count  Dtype         \n",
      "---  ------                        --------------  -----         \n",
      " 0   record_id                     20085 non-null  int64         \n",
      " 1   Timestamp                     20085 non-null  datetime64[ns]\n",
      " 2   Location                      20085 non-null  object        \n",
      " 3   Latitude                      20085 non-null  float64       \n",
      " 4   Longitude                     20085 non-null  float64       \n",
      " 5   Day_of_Week                   20085 non-null  object        \n",
      " 6   Hour_of_Day                   20085 non-null  int64         \n",
      " 7   Incident_Congestion           20085 non-null  object        \n",
      " 8   Weather_Condition             20085 non-null  object        \n",
      " 9   Road_Condition                20085 non-null  object        \n",
      " 10  Vehicle_Type                  20085 non-null  object        \n",
      " 11  Vehicle_Damage                18077 non-null  object        \n",
      " 12  Accident_Severity             20085 non-null  object        \n",
      " 13  Accident_Reason               20085 non-null  object        \n",
      " 14  Driver_Violation              15029 non-null  object        \n",
      " 15  Number_of_Deaths              20085 non-null  int64         \n",
      " 16  Number_of_Injuries            20085 non-null  int64         \n",
      " 17  Summary_Road_Name             20085 non-null  object        \n",
      " 18  Daily_Traffic_Volume          20085 non-null  int64         \n",
      " 19  Daily_Average_Speed           20085 non-null  float64       \n",
      " 20  Daily_Congestion_Level        20085 non-null  float64       \n",
      " 21  Daily_Incident_Reports        20085 non-null  int64         \n",
      " 22  Daily_Public_Transport_Usage  20085 non-null  float64       \n",
      " 23  Daily_Parking_Usage           20085 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(6), int64(6), object(11)\n",
      "memory usage: 3.7+ MB\n",
      "\n",
      "--- Missing Value Report (Before Imputation) ---\n",
      "record_id                          0\n",
      "Timestamp                          0\n",
      "Location                           0\n",
      "Latitude                           0\n",
      "Longitude                          0\n",
      "Day_of_Week                        0\n",
      "Hour_of_Day                        0\n",
      "Incident_Congestion                0\n",
      "Weather_Condition                  0\n",
      "Road_Condition                     0\n",
      "Vehicle_Type                       0\n",
      "Vehicle_Damage                  2008\n",
      "Accident_Severity                  0\n",
      "Accident_Reason                    0\n",
      "Driver_Violation                5056\n",
      "Number_of_Deaths                   0\n",
      "Number_of_Injuries                 0\n",
      "Summary_Road_Name                  0\n",
      "Daily_Traffic_Volume               0\n",
      "Daily_Average_Speed                0\n",
      "Daily_Congestion_Level             0\n",
      "Daily_Incident_Reports             0\n",
      "Daily_Public_Transport_Usage       0\n",
      "Daily_Parking_Usage                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- 1. Correcting Data Types ---\")\n",
    "\n",
    "# --- Convert Timestamp to datetime ---\n",
    "# This is critical for all temporal analysis in Phase 3\n",
    "try:\n",
    "    # Replace 'Timestamp' if your column name is different\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    print(\"'Timestamp' column converted to datetime.\")\n",
    "except KeyError:\n",
    "    print(\"ERROR: 'Timestamp' column not found. Please check the name.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠Warning: Could not convert 'Timestamp'. Error: {e}\")\n",
    "\n",
    "# --- Convert key numerical columns ---\n",
    "# We force errors to 'NaN' (Not a Number)\n",
    "numerical_cols = ['Latitude', 'Longitude', 'Number_of_Vehicles', 'Number_of_Casualties']\n",
    "for col in numerical_cols:\n",
    "    if col in df.columns:\n",
    "        original_type = df[col].dtype\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        print(f\"'{col}' column converted from {original_type} to numeric.\")\n",
    "    else:\n",
    "        print(f\"Info: Column '{col}' not found, skipping conversion.\")\n",
    "\n",
    "print(\"\\n--- 2. Marking Invalid Placeholders as NaN ---\")\n",
    "\n",
    "# --- Mark invalid Speed_Limit values ---\n",
    "# Your plan specifies -1 or 99 as invalid\n",
    "if 'Speed_Limit' in df.columns:\n",
    "    # First, ensure it's numeric (handles text errors)\n",
    "    df['Speed_Limit'] = pd.to_numeric(df['Speed_Limit'], errors='coerce')\n",
    "    \n",
    "    # Now, replace the invalid placeholders\n",
    "    invalid_speeds = [-1, 99] \n",
    "    original_nan_count = df['Speed_Limit'].isna().sum()\n",
    "    df['Speed_Limit'] = df['Speed_Limit'].replace(invalid_speeds, np.nan)\n",
    "    new_nan_count = df['Speed_Limit'].isna().sum()\n",
    "    print(f\"'Speed_Limit': Marked {new_nan_count - original_nan_count} invalid values as NaN.\")\n",
    "else:\n",
    "    print(\"ℹInfo: 'Speed_Limit' column not found, skipping.\")\n",
    "\n",
    "# --- Mark invalid Geocoordinates ---\n",
    "# Your plan mentions -1 or 0 as invalid\n",
    "if 'Latitude' in df.columns:\n",
    "    df['Latitude'] = df['Latitude'].replace([-1, 0], np.nan)\n",
    "    print(\"'Latitude': Marked -1 and 0 as NaN.\")\n",
    "if 'Longitude' in df.columns:\n",
    "    df['Longitude'] = df['Longitude'].replace([-1, 0], np.nan)\n",
    "    print(\"'Longitude': Marked -1 and 0 as NaN.\")\n",
    "\n",
    "print(\"\\n--- Data Types After Correction ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- Missing Value Report (Before Imputation) ---\")\n",
    "# This shows us exactly what we need to impute in the next step\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357cba6-62b0-444d-b619-aed2af299729",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________________________________\n",
    "Step 3: Advanced (Decision Tree) & Simple Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d254e858-7e10-4274-8d84-5b915b357d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Column 'YOUR_ACTUAL_SPEED_COLUMN' not in DataFrame. Skipping speed limit imputation.\n",
      "\n",
      "--- Advanced Imputation for 'Latitude' ---\n",
      " ERROR: No valid predictor features found for 'Latitude'. Skipping.\n",
      "\n",
      "--- Advanced Imputation for 'Longitude' ---\n",
      " ERROR: No valid predictor features found for 'Longitude'. Skipping.\n",
      "\n",
      "--- Simple Imputation for Remaining Columns ---\n",
      " Filled missing 'Driver_Violation' values with 'Unknown'.\n",
      " Filled missing 'Vehicle_Damage' values with 'Unknown'.\n",
      "\n",
      "--- Missing Value Report (After Imputation) ---\n",
      "record_id                       0\n",
      "Timestamp                       0\n",
      "Location                        0\n",
      "Latitude                        0\n",
      "Longitude                       0\n",
      "Day_of_Week                     0\n",
      "Hour_of_Day                     0\n",
      "Incident_Congestion             0\n",
      "Weather_Condition               0\n",
      "Road_Condition                  0\n",
      "Vehicle_Type                    0\n",
      "Vehicle_Damage                  0\n",
      "Accident_Severity               0\n",
      "Accident_Reason                 0\n",
      "Driver_Violation                0\n",
      "Number_of_Deaths                0\n",
      "Number_of_Injuries              0\n",
      "Summary_Road_Name               0\n",
      "Daily_Traffic_Volume            0\n",
      "Daily_Average_Speed             0\n",
      "Daily_Congestion_Level          0\n",
      "Daily_Incident_Reports          0\n",
      "Daily_Public_Transport_Usage    0\n",
      "Daily_Parking_Usage             0\n",
      "Day_of_Week_Name                0\n",
      "Time_of_Day                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def impute_with_decision_tree(df, target_col, predictor_features):\n",
    "    \"\"\"\n",
    "    Imputes a target column using Decision Tree Regression.\n",
    "    Predictor features must be categorical for OneHotEncoding.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Advanced Imputation for '{target_col}' ---\")\n",
    "    \n",
    "    # Ensures predictor features exist\n",
    "    valid_predictors = [col for col in predictor_features if col in df.columns]\n",
    "    if not valid_predictors:\n",
    "        print(f\" ERROR: No valid predictor features found for '{target_col}'. Skipping.\")\n",
    "        return df\n",
    "    \n",
    "    # Create a copy to avoid changing original data during fitting\n",
    "    impute_df = df[valid_predictors + [target_col]].copy()\n",
    "    \n",
    "    # Filling NaNs in predictors with 'Unknown'\n",
    "    for col in valid_predictors:\n",
    "        impute_df[col] = impute_df[col].fillna('Unknown').astype(str)\n",
    "        \n",
    "    # 1. Prepare data\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    \n",
    "    # Fit encoder on ALL predictor data (train + predict)\n",
    "    encoder.fit(impute_df[valid_predictors])\n",
    "    \n",
    "    # 2. Split data\n",
    "    train_data = impute_df.dropna(subset=[target_col])\n",
    "    predict_data = impute_df[impute_df[target_col].isna()]\n",
    "    \n",
    "    if predict_data.empty:\n",
    "        print(f\" No missing '{target_col}' values to impute.\")\n",
    "        return df\n",
    "        \n",
    "    if train_data.empty:\n",
    "        print(f\" ERROR: No training data (known values) for '{target_col}'. Cannot impute.\")\n",
    "        return df\n",
    "        \n",
    "    X_train = encoder.transform(train_data[valid_predictors])\n",
    "    y_train = train_data[target_col]\n",
    "    X_predict = encoder.transform(predict_data[valid_predictors])\n",
    "    \n",
    "    # 3. Train model\n",
    "    print(f\"Training Decision Tree model for '{target_col}'...\")\n",
    "    dt_reg = DecisionTreeRegressor(random_state=42, max_depth=10) # max_depth to prevent overfitting\n",
    "    dt_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # 4. Predict and fill\n",
    "    predicted_values = dt_reg.predict(X_predict)\n",
    "    \n",
    "    # Fill the original DataFrame\n",
    "    df.loc[df[target_col].isna(), target_col] = predicted_values\n",
    "    print(f\" Successfully imputed {len(predicted_values)} missing '{target_col}' values.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- 1. Impute Speed_Limit ---\n",
    "speed_col_name = 'YOUR_ACTUAL_SPEED_COLUMN' # e.g., 'speed_limit' or 'SpeedLimit'\n",
    "\n",
    "speed_predictors = ['Road_Type', 'Day_of_Week_Name', 'Time_of_Day']\n",
    "\n",
    "if 'Timestamp' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Timestamp']):\n",
    "    df['Day_of_Week_Name'] = df['Timestamp'].dt.day_name()\n",
    "    df['Hour_of_Day'] = df['Timestamp'].dt.hour\n",
    "    \n",
    "    def get_time_of_day(hour):\n",
    "        if 5 <= hour < 12: return 'Morning'\n",
    "        elif 12 <= hour < 17: return 'Afternoon'\n",
    "        elif 17 <= hour < 21: return 'Evening'\n",
    "        else: return 'Night'\n",
    "    df['Time_of_Day'] = df['Hour_of_Day'].apply(get_time_of_day).fillna('Night')\n",
    "else:\n",
    "    print(\" Warning: Timestamp features not available. Imputation accuracy may be low.\")\n",
    "    speed_predictors = ['Road_Type']\n",
    "\n",
    "if speed_col_name in df.columns:\n",
    "    df = impute_with_decision_tree(df, speed_col_name, speed_predictors)\n",
    "else:\n",
    "    print(f\"ERROR: Column '{speed_col_name}' not in DataFrame. Skipping speed limit imputation.\")\n",
    "\n",
    "# --- 2. Impute Geocoordinates ---\n",
    "geo_predictors = ['Police_Force', 'Local_Authority_(District)', 'Road_Type'] # Check these names too!\n",
    "\n",
    "if 'Latitude' in df.columns:\n",
    "    df = impute_with_decision_tree(df, 'Latitude', geo_predictors)\n",
    "else:\n",
    "    print(\" ERROR: Column 'Latitude' not in DataFrame. Skipping imputation.\")\n",
    "\n",
    "if 'Longitude' in df.columns:\n",
    "    df = impute_with_decision_tree(df, 'Longitude', geo_predictors)\n",
    "else:\n",
    "    print(\" ERROR: Column 'Longitude' not in DataFrame. Skipping imputation.\")\n",
    "\n",
    "\n",
    "# --- Part 3B: Simple Imputation ---\n",
    "print(\"\\n--- Simple Imputation for Remaining Columns ---\")\n",
    "\n",
    "categorical_cols = ['Driver_Violation', 'Vehicle_Damage', 'Weather_Conditions', 'Road_Surface_Conditions']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "        print(f\" Filled missing '{col}' values with 'Unknown'.\")\n",
    "\n",
    "numerical_count_cols = ['Number_of_Vehicles', 'Number_of_Casualties']\n",
    "for col in numerical_count_cols:\n",
    "    if col in df.columns:\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        print(f\" Filled missing '{col}' values with median ({median_val}).\")\n",
    "\n",
    "# --- Final Check ---\n",
    "print(\"\\n--- Missing Value Report (After Imputation) ---\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d2931-d1b9-47d0-8880-3f9c5ca51f51",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________________________________\n",
    "Step 4: Feature Engineering & Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8303f39-12fe-4375-a0d6-fd167c865680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4A: Engineering Temporal Features (for EDA) ---\n",
      "My new temporal features are: 'Day_of_Week_Num', 'Month', 'Is_Weekend'\n",
      "\n",
      "--- 4B: Engineering Discretized Features (for Apriori) ---\n",
      "Warning: Column 'YOUR_ACTUAL_SPEED_COLUMN' not found. Skipping 'Speed_Limit_Bin'.\n",
      "Ensuring all Apriori columns are string type...\n",
      "Warning: Apriori column 'Light_Conditions' not found.\n",
      "Warning: Apriori column 'Weather_Conditions' not found.\n",
      "Warning: Apriori column 'Road_Surface_Conditions' not found.\n",
      "Warning: Apriori column 'Junction_Control' not found.\n",
      "\n",
      "\n",
      "--- FINAL DATA QUALITY REPORT (Phase 2 Complete) ---\n",
      "\n",
      "--- Final Data Info & Types ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20085 entries, 0 to 20084\n",
      "Data columns (total 29 columns):\n",
      " #   Column                        Non-Null Count  Dtype         \n",
      "---  ------                        --------------  -----         \n",
      " 0   record_id                     20085 non-null  int64         \n",
      " 1   Timestamp                     20085 non-null  datetime64[ns]\n",
      " 2   Location                      20085 non-null  object        \n",
      " 3   Latitude                      20085 non-null  float64       \n",
      " 4   Longitude                     20085 non-null  float64       \n",
      " 5   Day_of_Week                   20085 non-null  object        \n",
      " 6   Hour_of_Day                   20085 non-null  int32         \n",
      " 7   Incident_Congestion           20085 non-null  object        \n",
      " 8   Weather_Condition             20085 non-null  object        \n",
      " 9   Road_Condition                20085 non-null  object        \n",
      " 10  Vehicle_Type                  20085 non-null  object        \n",
      " 11  Vehicle_Damage                20085 non-null  object        \n",
      " 12  Accident_Severity             20085 non-null  object        \n",
      " 13  Accident_Reason               20085 non-null  object        \n",
      " 14  Driver_Violation              20085 non-null  object        \n",
      " 15  Number_of_Deaths              20085 non-null  int64         \n",
      " 16  Number_of_Injuries            20085 non-null  int64         \n",
      " 17  Summary_Road_Name             20085 non-null  object        \n",
      " 18  Daily_Traffic_Volume          20085 non-null  int64         \n",
      " 19  Daily_Average_Speed           20085 non-null  float64       \n",
      " 20  Daily_Congestion_Level        20085 non-null  float64       \n",
      " 21  Daily_Incident_Reports        20085 non-null  int64         \n",
      " 22  Daily_Public_Transport_Usage  20085 non-null  float64       \n",
      " 23  Daily_Parking_Usage           20085 non-null  float64       \n",
      " 24  Day_of_Week_Name              20085 non-null  object        \n",
      " 25  Time_of_Day                   20085 non-null  object        \n",
      " 26  Day_of_Week_Num               20085 non-null  int32         \n",
      " 27  Month                         20085 non-null  int32         \n",
      " 28  Is_Weekend                    20085 non-null  int32         \n",
      "dtypes: datetime64[ns](1), float64(6), int32(4), int64(5), object(13)\n",
      "memory usage: 4.1+ MB\n",
      "\n",
      "--- Final Missing Values Check ---\n",
      "My dataset has no missing values remaining.\n",
      "\n",
      "Phase 2 is complete. My cleaned data is saved to 'cleaned_for_phase_3.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"--- 4A: Engineering Temporal Features (for EDA) ---\")\n",
    "\n",
    "if 'Timestamp' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Timestamp']):\n",
    "    # I already created Hour_of_Day and Day_of_Week_Name in Step 3\n",
    "    # Now I'll create the remaining temporal features I need\n",
    "    \n",
    "    # I'll create a numeric day-of-week (0=Monday) for sorting\n",
    "    if 'Day_of_Week_Num' not in df.columns:\n",
    "        df['Day_of_Week_Num'] = df['Timestamp'].dt.dayofweek\n",
    "    \n",
    "    # I'll extract the Month\n",
    "    if 'Month' not in df.columns:\n",
    "        df['Month'] = df['Timestamp'].dt.month\n",
    "    \n",
    "    # I'll create a binary 'Is_Weekend' feature\n",
    "    if 'Is_Weekend' not in df.columns:\n",
    "        df['Is_Weekend'] = df['Day_of_Week_Num'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    print(\"My new temporal features are: 'Day_of_Week_Num', 'Month', 'Is_Weekend'\")\n",
    "else:\n",
    "    print(\"Warning: 'Timestamp' column not found or not datetime. Skipping temporal features.\")\n",
    "\n",
    "# --- 4B: Engineering Discretized Features (for Apriori) ---\n",
    "print(\"\\n--- 4B: Engineering Discretized Features (for Apriori) ---\")\n",
    "\n",
    "# 1. I need to discretize 'Speed_Limit'\n",
    "# !!! IMPORTANT: I must replace 'YOUR_ACTUAL_SPEED_COLUMN' with my real column name\n",
    "speed_col_name = 'YOUR_ACTUAL_SPEED_COLUMN' # e.g., 'Speed_Limit'\n",
    "\n",
    "if speed_col_name in df.columns:\n",
    "    # I'll create string-based bins for my Apriori analysis\n",
    "    speed_bins = [0, 30, 50, 70, 100] # Bins: 0-29, 30-49, 50-69, 70-99\n",
    "    speed_labels = ['Speed_0-29', 'Speed_30-49', 'Speed_50-69', 'Speed_70-99']\n",
    "    \n",
    "    df['Speed_Limit_Bin'] = pd.cut(df[speed_col_name],\n",
    "                                 bins=speed_bins,\n",
    "                                 labels=speed_labels,\n",
    "                                 right=False) # 'right=False' means [0-30)\n",
    "    \n",
    "    # I'll convert this to a string and fill any NaNs\n",
    "    df['Speed_Limit_Bin'] = df['Speed_Limit_Bin'].astype(str).replace('nan', 'Speed_Unknown')\n",
    "    print(f\"Created 'Speed_Limit_Bin' from '{speed_col_name}'.\")\n",
    "else:\n",
    "    print(f\"Warning: Column '{speed_col_name}' not found. Skipping 'Speed_Limit_Bin'.\")\n",
    "\n",
    "# 2. I'll ensure all my other Apriori features are clean strings\n",
    "# (I mostly did this in Step 3, but this confirms it)\n",
    "apriori_cols = ['Accident_Severity', 'Time_of_Day', 'Day_of_Week_Name', \n",
    "                'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions', \n",
    "                'Junction_Control', 'Vehicle_Damage']\n",
    "\n",
    "print(\"Ensuring all Apriori columns are string type...\")\n",
    "for col in apriori_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    else:\n",
    "        print(f\"Warning: Apriori column '{col}' not found.\")\n",
    "\n",
    "\n",
    "# --- 4C: Final Validation & Save ---\n",
    "print(\"\\n\\n--- FINAL DATA QUALITY REPORT (Phase 2 Complete) ---\")\n",
    "\n",
    "print(\"\\n--- Final Data Info & Types ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- Final Missing Values Check ---\")\n",
    "final_missing = df.isna().sum()\n",
    "if final_missing.sum() == 0:\n",
    "    print(\"My dataset has no missing values remaining.\")\n",
    "else:\n",
    "    print(\"Warning: Missing values are still present:\")\n",
    "    print(final_missing[final_missing > 0])\n",
    "\n",
    "# 4. I'll save the fully cleaned and feature-engineered dataset\n",
    "cleaned_file_name = \"cleaned_for_phase_3.csv\"\n",
    "df.to_csv(cleaned_file_name, index=False)\n",
    "print(f\"\\nPhase 2 is complete. My cleaned data is saved to '{cleaned_file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd67fa4-1292-4b96-b4c2-b6f0b6599675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
